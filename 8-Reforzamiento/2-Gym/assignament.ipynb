{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entrena el carrito de montaña\n",
    "\n",
    "[OpenAI Gym](http://gym.openai.com) ha sido diseñado de tal forma que todos los ambientes proveen la misma API - esto es, los mismos métodos `reset`, `step` y `render`, y las mismas abstracciones de **action space** y **observation space**. Así sería posible adaptar los mismos algoritmos de aprendizaje reforzado a diferentes ambientes con mínimos cambios al código."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instrucciones\n",
    "\n",
    "Adapta nuestro algoritmo de aprendizaje reforzado para resolver el problema del carrito de montaña. Comienza con el código existente en [notebook.ipynb](../notebook.ipynb), substituye el nuevo ambiente, cambia las funciones de discretización de estado, e intenta hacer que el algoritmo existente entrene con mínimas modificaciones al código. Optimiza el resultado al ajustar los hiperparámetros."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Instalar OpenAI Gym:\n",
    "Primero, asegúrate de tener gym instalado.\n",
    "\n",
    "2. Importa las librerías necesarias y define el entorno de MountainCar-v0 que ya está disponible en Gym."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gym in c:\\users\\hp\\anaconda3\\lib\\site-packages (0.26.2)\n",
      "Requirement already satisfied: numpy>=1.18.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from gym) (1.24.3)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from gym) (2.2.1)\n",
      "Requirement already satisfied: gym-notices>=0.0.4 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from gym) (0.0.8)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install gym \n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Definición del entorno: Utiliza MountainCar-v0 de OpenAI Gym.\n",
    "\n",
    "4. Discretización del espacio de observación: Dado que el espacio de observación es continuo, lo discretizamos utilizando bins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear el entorno de MountainCar-v0\n",
    "env = gym.make('MountainCar-v0')\n",
    "\n",
    "# Definir el rango de discretización para las observaciones\n",
    "position_bins = np.linspace(-1.2, 0.6, 20)\n",
    "velocity_bins = np.linspace(-0.07, 0.07, 20)\n",
    "\n",
    "def discretize(obs):\n",
    "    position, velocity = obs\n",
    "    position_idx = np.digitize(position, position_bins)\n",
    "    velocity_idx = np.digitize(velocity, velocity_bins)\n",
    "    return position_idx, velocity_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Definición del algoritmo de Q-learning: Se define el algoritmo de Q-learning con una Q-Tabla inicializada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir los parámetros de Q-learning\n",
    "alpha = 0.1  # Tasa de aprendizaje\n",
    "gamma = 0.99  # Factor de descuento\n",
    "epsilon = 0.1  # Tasa de exploración\n",
    "episodes = 10000  # Número de episodios de entrenamiento\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializar la Q-Tabla\n",
    "Q = defaultdict(float)\n",
    "\n",
    "# Definir la función para elegir una acción\n",
    "def choose_action(state, epsilon):\n",
    "    if random.random() < epsilon:\n",
    "        return env.action_space.sample()\n",
    "    else:\n",
    "        q_values = [Q[(state, a)] for a in range(env.action_space.n)]\n",
    "        return np.argmax(q_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Entrenamiento: Entrena el agente durante un número específico de episodios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\HP\\anaconda3\\Lib\\site-packages\\gym\\utils\\passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0, Total Reward: -10857\n",
      "Episode: 1000, Total Reward: -508\n",
      "Episode: 2000, Total Reward: -153\n",
      "Episode: 3000, Total Reward: -163\n",
      "Episode: 4000, Total Reward: -213\n",
      "Episode: 5000, Total Reward: -195\n",
      "Episode: 6000, Total Reward: -184\n",
      "Episode: 7000, Total Reward: -189\n",
      "Episode: 8000, Total Reward: -210\n",
      "Episode: 9000, Total Reward: -223\n"
     ]
    }
   ],
   "source": [
    "# Entrenar el agente con Q-learning\n",
    "# Entrenar el agente con Q-learning\n",
    "for episode in range(episodes):\n",
    "    obs, _ = env.reset()\n",
    "    state = discretize(obs)\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "\n",
    "    while not done:\n",
    "        action = choose_action(state, epsilon)\n",
    "        next_obs, reward, done, truncated, info = env.step(action)\n",
    "        next_state = discretize(next_obs)\n",
    "\n",
    "        if next_obs[0] >= 0.5:\n",
    "            reward = 0\n",
    "        else:\n",
    "            reward = -1\n",
    "\n",
    "        best_next_action = np.argmax([Q[(next_state, a)] for a in range(env.action_space.n)])\n",
    "        td_target = reward + gamma * Q[(next_state, best_next_action)]\n",
    "        Q[(state, action)] += alpha * (td_target - Q[(state, action)])\n",
    "\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "\n",
    "    if episode % 1000 == 0:\n",
    "        print(f\"Episode: {episode}, Total Reward: {total_reward}\")\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Evaluación: Evalúa la política aprendida durante unos pocos episodios, renderizando el entorno para observar el comportamiento del agente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0, Total Reward: -96.0\n",
      "Episode: 1, Total Reward: -138.0\n",
      "Episode: 2, Total Reward: -143.0\n",
      "Episode: 3, Total Reward: -125.0\n",
      "Episode: 4, Total Reward: -230.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\HP\\anaconda3\\Lib\\site-packages\\gym\\envs\\classic_control\\mountain_car.py:171: UserWarning: \u001b[33mWARN: You are calling render method without specifying any render mode. You can specify the render_mode at initialization, e.g. gym(\"MountainCar-v0\", render_mode=\"rgb_array\")\u001b[0m\n",
      "  gym.logger.warn(\n"
     ]
    }
   ],
   "source": [
    "# Evaluar la política aprendida\n",
    "for episode in range(5):\n",
    "    obs, _ = env.reset()\n",
    "    state = discretize(obs)\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "\n",
    "    while not done:\n",
    "        env.render()\n",
    "        action = choose_action(state, 0)  # Exploit only\n",
    "        next_obs, reward, done, truncated, info = env.step(action)\n",
    "        state = discretize(next_obs)\n",
    "        total_reward += reward\n",
    "\n",
    "    print(f\"Episode: {episode}, Total Reward: {total_reward}\")\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusión\n",
    "En este ejercicio, se implemento y entreno un agente utilizando el algoritmo de Q-learning en el entorno MountainCar-v0 de **OpenAI** Gym. El objetivo del agente es aprender a controlar un carrito atrapado en un valle para alcanzar una bandera en la cima de la montaña.\n",
    "\n",
    "* Este ejercicio demostró la eficacia del algoritmo de Q-learning en entornos con espacios de estado discretos. La discretización del espacio de observación permitió manejar la naturaleza continua del entorno MountainCar-v0.\n",
    "* La política epsilon-greedy equilibró adecuadamente la exploración de nuevas acciones con la explotación del conocimiento adquirido, lo cual fue crucial para el aprendizaje efectivo del agente.\n",
    "* Una correcta discretización del espacio de observación fue fundamental para el éxito del agente. Una discretización muy gruesa podría haber limitado la capacidad del agente para aprender diferencias sutiles en el entorno, mientras que una demasiado fina podría haber conducido a una explosión en el tamaño de la Q-Tabla.\n",
    "* El ejercicio también destacó la necesidad de adaptar el código a las actualizaciones de la API de OpenAI Gym, particularmente en cómo se manejan los retornos de la función env.step().\n",
    "\n",
    "En resumen, este ejercicio proporcionó una comprensión práctica de cómo implementar y ajustar el Q-learning en un entorno de simulación, destacando tanto los desafíos como las soluciones efectivas en el proceso de aprendizaje por refuerzo."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
